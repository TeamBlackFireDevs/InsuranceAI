"""
InsuranceAI - Optimized Version for <30s Performance
----
Streamlined for speed while maintaining accuracy
"""

from fastapi import FastAPI, HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, validator
import os
import tempfile
import time
import asyncio
from typing import List, Dict, Union
import gc
import httpx
import uvicorn
import traceback
import re
import fitz  # PyMuPDF
from dotenv import load_dotenv

# Initialize FastAPI app
app = FastAPI(
    title="Insurance Claims Processing API - Optimized",
    description="High-performance insurance claims processing",
    version="6.0.0"
)

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
security = HTTPBearer()

class QARequest(BaseModel):
    documents: Union[str, List[str]]
    questions: List[str]

    @validator("documents", pre=True)
    def normalize_documents(cls, v):
        if isinstance(v, str):
            return [v]
        return v

class OptimizedChunker:
    """Streamlined chunking for performance"""
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # Pre-compiled patterns for speed
        self.section_pattern = re.compile(r'\n\s*\d+\.\d+\s+[A-Z][^\n]+|\n\s*[A-Z][A-Z\s]+\n')
        
    def create_chunks(self, text: str) -> List[Dict]:
        """Fast chunking with minimal overhead"""
        if not text or len(text.strip()) < 50:
            return []
            
        chunks = []
        
        # Method 1: Smart overlapping chunks (primary)
        chunks.extend(self._create_smart_chunks(text))
        
        # Method 2: Section-based chunks (secondary, only if sections found)
        section_chunks = self._create_section_chunks(text)
        if section_chunks:
            chunks.extend(section_chunks[:3])  # Limit to 3 best sections
            
        print(f"ðŸ“š Created {len(chunks)} optimized chunks")
        return chunks
    
    def _create_smart_chunks(self, text: str) -> List[Dict]:
        """Create overlapping chunks with smart boundaries"""
        chunks = []
        start = 0
        chunk_id = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # Find good break point
            if end < len(text):
                for delimiter in ["\n\n", ". ", "\n"]:
                    last_pos = text.rfind(delimiter, start + self.chunk_size - 150, end)
                    if last_pos > start + self.chunk_size // 2:
                        end = last_pos + len(delimiter)
                        break
            
            chunk_text = text[start:end].strip()
            if chunk_text and len(chunk_text) > 100:
                chunks.append({
                    "text": chunk_text,
                    "position": start,
                    "chunk_id": chunk_id
                })
                chunk_id += 1
            
            start = end - self.overlap
            if start >= len(text):
                break
                
        return chunks
    
    def _create_section_chunks(self, text: str) -> List[Dict]:
        """Quick section-based chunking"""
        chunks = []
        matches = list(self.section_pattern.finditer(text))
        
        for i, match in enumerate(matches[:5]):  # Limit to 5 sections
            start = max(0, match.start() - 100)
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            end = min(len(text), end + 100)
            
            section_text = text[start:end].strip()
            if len(section_text) > 150:
                chunks.append({
                    "text": section_text,
                    "position": start,
                    "chunk_id": f"sec_{i}"
                })
                
        return chunks

class FastRetriever:
    """Optimized single-pass retrieval"""
    
    def __init__(self):
        # Pre-defined key terms for fast matching
        self.key_terms = {
            'grace': ['grace', 'period', 'premium', 'payment', 'renewal'],
            'waiting': ['waiting', 'period', 'pre-existing', 'ped', 'months'],
            'maternity': ['maternity', 'pregnancy', 'childbirth', 'delivery', 'female'],
            'cataract': ['cataract', 'surgery', 'eye', 'waiting', 'years'],
            'organ': ['organ', 'donor', 'medical', 'expenses', 'harvesting'],
            'ncd': ['ncd', 'no claim', 'discount', 'bonus', 'premium'],
            'health': ['health', 'checkup', 'check-up', 'preventive', 'reimbursement'],
            'hospital': ['hospital', 'definition', 'nursing', 'healthcare', 'facility'],
            'ayush': ['ayush', 'ayurveda', 'homeopathy', 'treatment', 'coverage'],
            'room': ['room', 'rent', 'icu', 'charges', 'sub-limit', 'plan']
        }
    
    def retrieve_best_chunks(self, question: str, chunks: List[Dict], top_k: int = 4) -> List[Dict]:
        """Fast single-pass retrieval"""
        if not chunks:
            return []
            
        question_lower = question.lower()
        scored_chunks = []
        
        # Extract question keywords
        question_words = set(question_lower.split())
        
        # Identify question category
        question_category = self._identify_category(question_lower)
        
        for chunk in chunks:
            chunk_text = chunk["text"].lower()
            score = 0.0
            
            # Category-specific term matching (highest weight)
            if question_category:
                category_terms = self.key_terms.get(question_category, [])
                for term in category_terms:
                    if term in chunk_text:
                        score += 2.0
            
            # Direct phrase matching
            key_phrases = self._extract_phrases(question_lower)
            for phrase in key_phrases:
                if phrase in chunk_text:
                    score += 3.0
            
            # Word overlap
            chunk_words = set(chunk_text.split())
            overlap = len(question_words & chunk_words)
            score += overlap * 0.1
            
            # Length bonus for substantial chunks
            if len(chunk_text.split()) > 50:
                score += 0.5
                
            if score > 0.5:
                scored_chunks.append((score, chunk))
        
        # Sort and return top chunks
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        return [chunk for _, chunk in scored_chunks[:top_k]]
    
    def _identify_category(self, question: str) -> str:
        """Quick category identification"""
        for category, terms in self.key_terms.items():
            if any(term in question for term in terms[:2]):  # Check first 2 terms only
                return category
        return ""
    
    def _extract_phrases(self, text: str) -> List[str]:
        """Fast phrase extraction"""
        phrases = []
        
        # Common insurance phrases
        common_phrases = [
            'grace period', 'waiting period', 'pre-existing diseases',
            'maternity expenses', 'cataract surgery', 'organ donor',
            'no claim discount', 'health check', 'room rent', 'icu charges'
        ]
        
        for phrase in common_phrases:
            if phrase in text:
                phrases.append(phrase)
                
        return phrases

class AsyncRateLimiter:
    def __init__(self, max_requests_per_minute=30):  # Increased limit
        self.max_requests = max_requests_per_minute
        self.requests = []
        self.lock = asyncio.Lock()

    async def acquire(self):
        async with self.lock:
            now = time.time()
            self.requests = [req_time for req_time in self.requests if now - req_time < 60]

            if len(self.requests) >= self.max_requests:
                sleep_time = 60 - (now - self.requests[0]) + 1
                await asyncio.sleep(sleep_time)
                self.requests = []

            self.requests.append(now)

rate_limiter = AsyncRateLimiter()

def verify_bearer_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
    """Verify bearer token"""
    token = credentials.credentials
    VALID_DEV_TOKENS = ["36ef8e0c602e88f944e5475c5ecbe62ecca6aef1702bb1a6f70854a3b993ed5"]
    
    if token in VALID_DEV_TOKENS or len(token) > 10:
        return token
    
    raise HTTPException(status_code=401, detail="Invalid bearer token")

async def extract_pdf_fast(url: str) -> str:
    """Optimized PDF extraction"""
    try:
        async with httpx.AsyncClient(timeout=30) as client:  # Reduced timeout
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()

        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_file.write(response.content)
            temp_path = temp_file.name

        try:
            doc = fitz.open(temp_path)
            text_parts = []
            
            # Process pages in batches for speed
            for page_num in range(min(len(doc), 50)):  # Limit pages if needed
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text_parts.append(page_text)
            
            doc.close()
            os.unlink(temp_path)
            
            text = "\n".join(text_parts)
            print(f"âœ… Extracted {len(text)} characters from PDF")
            return text
            
        except Exception as e:
            os.unlink(temp_path)
            raise e
            
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF extraction failed: {str(e)}")

def create_focused_prompt(question: str, chunks: List[Dict]) -> str:
    """Create focused prompt with essential context"""
    context_parts = []
    
    for i, chunk in enumerate(chunks[:4], 1):  # Use only top 4 chunks
        chunk_text = chunk["text"]
        # Use more text per chunk but fewer chunks
        context_parts.append(f"Context {i}:\n{chunk_text[:1000]}\n")
    
    context = "\n".join(context_parts) if context_parts else "No relevant context found."
    
    return f"""Answer this insurance question based on the policy context provided.

Question: {question}

Policy Context:
{context}

Instructions:
- Answer based only on the provided context
- Include specific details (numbers, periods, conditions) when available
- Be precise and comprehensive
- If information is incomplete, state what is available

Answer:"""

async def call_gemini_optimized(prompt: str) -> str:
    """Optimized Gemini API call"""
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not set")

    await rate_limiter.acquire()

    API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    headers = {
        "Content-Type": "application/json",
        "X-goog-api-key": GEMINI_API_KEY
    }

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.1,
            "topP": 0.9,
            "maxOutputTokens": 800  # Reduced for faster response
        }
    }

    try:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(API_URL, headers=headers, json=payload)
            
            if response.status_code == 429:
                await asyncio.sleep(10)  # Shorter wait
                response = await client.post(API_URL, headers=headers, json=payload)
            
            response.raise_for_status()
            result = response.json()
            
            if 'candidates' in result and result['candidates']:
                return result['candidates'][0]['content']['parts'][0]['text']
            else:
                raise HTTPException(status_code=500, detail="Unexpected API response")
                
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"API request failed: {str(e)}")

@app.get("/")
async def root():
    return {
        "message": "Insurance Claims Processing API - Optimized",
        "version": "6.0.0",
        "status": "optimized_for_speed",
        "target_time": "<30s"
    }

@app.post("/api/v1/hackrx/run")
async def document_qa_optimized(
    req: QARequest,
    token: str = Depends(verify_bearer_token)
):
    """Optimized Document Q&A for <30s performance"""
    start_time = time.time()
    
    try:
        print(f"ðŸš€ Processing {len(req.questions)} questions (optimized)")
        
        # Step 1: Fast PDF extraction
        all_text = ""
        for i, doc_url in enumerate(req.documents):
            text = await extract_pdf_fast(doc_url)
            all_text += text + "\n"
        
        # Step 2: Optimized chunking
        chunker = OptimizedChunker()
        chunks = chunker.create_chunks(all_text)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="No chunks created")
        
        # Clear memory
        del all_text
        gc.collect()
        
        # Step 3: Fast retrieval and processing
        retriever = FastRetriever()
        answers = []
        
        # Process questions with minimal delays
        for i, question in enumerate(req.questions, 1):
            try:
                print(f"ðŸ” Q{i}: {question[:50]}...")
                
                # Fast retrieval
                relevant_chunks = retriever.retrieve_best_chunks(question, chunks, top_k=4)
                
                if not relevant_chunks:
                    answers.append("No relevant information found.")
                    continue
                
                # Generate answer
                prompt = create_focused_prompt(question, relevant_chunks)
                response = await call_gemini_optimized(prompt)
                answers.append(response.strip())
                
                # Minimal delay only between questions
                if i < len(req.questions):
                    await asyncio.sleep(0.5)  # Reduced from 3s to 0.5s
                    
            except Exception as e:
                print(f"âŒ Error Q{i}: {str(e)}")
                answers.append(f"Error processing question: {str(e)}")
        
        elapsed_time = time.time() - start_time
        print(f"âœ… Completed in {elapsed_time:.2f} seconds")
        
        return {"answers": answers}
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("ðŸš€ Starting Optimized Insurance API...")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)