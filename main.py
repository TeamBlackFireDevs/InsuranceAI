"""
InsuranceAI - Enhanced PDF Parsing & Retrieval
----
Key improvements:
1. Complete document processing (all pages)
2. Better chunking with semantic boundaries
3. Enhanced keyword matching for insurance terms
4. Multi-level retrieval strategy
5. Improved context assembly
6. Better handling of specific terms like "grace period"
"""

from fastapi import FastAPI, HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, validator
import os
import time
import asyncio
from typing import List, Dict, Union
import gc
import requests
import uvicorn
import traceback
import re
import fitz  # PyMuPDF
import httpx
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
import threading

# Initialize FastAPI app
app = FastAPI(
    title="Insurance Claims Processing API - Enhanced",
    description="Enhanced insurance claims processing with improved PDF parsing",
    version="4.0.0"
)

load_dotenv()
LLM_KEY = os.getenv("HF_TOKEN")
security = HTTPBearer()

# Global HTTP client for connection reuse
http_client = None
executor = ThreadPoolExecutor(max_workers=4)

class QARequest(BaseModel):
    documents: Union[str, List[str]]
    questions: List[str]

    @validator("documents", pre=True)
    def normalize_documents(cls, v):
        if isinstance(v, str):
            return [v]
        return v

class SpeedRateLimiter:
    def __init__(self, max_requests_per_minute=60):
        self.max_requests = max_requests_per_minute
        self.requests = []
        self.lock = threading.Lock()

    def acquire(self):
        with self.lock:
            now = time.time()
            self.requests = [req_time for req_time in self.requests if now - req_time < 60]

            if len(self.requests) >= self.max_requests:
                sleep_time = 2
                time.sleep(sleep_time)
                self.requests = []

            self.requests.append(now)

rate_limiter = SpeedRateLimiter()

def verify_bearer_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
    """Fast token verification"""
    token = credentials.credentials

    VALID_DEV_TOKENS = [
        "36ef8e0c602e88f944e5475c5ecbe62ecca6aef1702bb1a6f70854a3b993ed5"
    ]

    if token in VALID_DEV_TOKENS or len(token) > 10:
        return token

    raise HTTPException(status_code=401, detail="Invalid bearer token")

async def get_http_client():
    """Get or create HTTP client with connection pooling"""
    global http_client
    if http_client is None:
        http_client = httpx.AsyncClient(
            timeout=30,  # Increased for complete processing
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
        )
    return http_client

async def extract_pdf_enhanced(url: str) -> str:
    """Enhanced PDF extraction - processes ALL pages with better text extraction"""
    try:
        client = await get_http_client()
        response = await client.get(url, follow_redirects=True)
        response.raise_for_status()

        # Enhanced text extraction with PyMuPDF
        with fitz.open(stream=response.content, filetype="pdf") as doc:
            text_parts = []

            # Process ALL pages for complete coverage
            for page_num in range(len(doc)):
                page = doc[page_num]

                # Enhanced text extraction with better formatting
                text = page.get_text()

                # Clean up text while preserving structure
                text = re.sub(r'\n+', '\n', text)  # Normalize line breaks
                text = re.sub(r'\s+', ' ', text)    # Normalize spaces

                if text.strip():  # Only add non-empty pages
                    text_parts.append(f"--- Page {page_num + 1} ---\n{text}")

            full_text = "\n".join(text_parts)

            # Additional cleanup
            full_text = re.sub(r'\n\s*\n\s*\n', '\n\n', full_text)  # Remove excessive line breaks

            return full_text

    except Exception as e:
        raise HTTPException(status_code=400, detail=f"PDF extraction failed: {str(e)}")

def enhanced_chunking(text: str, chunk_size: int = 1500, overlap: int = 300) -> List[str]:
    """Enhanced chunking with better semantic boundaries"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))

        # Enhanced boundary detection with multiple delimiters
        if end < len(text):
            # Try different delimiters in order of preference
            delimiters = [
                "\n\n",           # Paragraph breaks
                ". ",              # Sentence endings
                "\n",             # Line breaks
                ", ",              # Comma breaks
                " "                # Word breaks (last resort)
            ]

            best_end = end
            for delimiter in delimiters:
                # Look for delimiter within reasonable range
                search_start = max(start + chunk_size - 400, start + chunk_size // 2)
                last_pos = text.rfind(delimiter, search_start, end + 100)

                if last_pos > search_start:
                    best_end = last_pos + len(delimiter)
                    break

            end = best_end

        chunk = text[start:end].strip()
        if chunk and len(chunk) > 50:  # Only add substantial chunks
            chunks.append(chunk)

        # Smart overlap to avoid cutting important information
        start = max(end - overlap, start + chunk_size // 2)
        if start >= len(text):
            break

    return chunks

def enhanced_chunk_scoring(question: str, chunk: str) -> float:
    """Enhanced chunk scoring with better keyword matching and insurance-specific terms"""
    q_lower = question.lower()
    c_lower = chunk.lower()

    # Enhanced keyword extraction
    q_words = set(re.findall(r'\b\w{2,}\b', q_lower))  # Include 2-letter words
    if not q_words:
        return 0.0

    # Basic keyword matching
    score = 0
    matched_words = 0
    for word in q_words:
        if word in c_lower:
            score += 1
            matched_words += 1

    base_score = score / len(q_words) if q_words else 0

    # Enhanced insurance term boosting with more specific patterns
    insurance_patterns = {
        'grace': [
            r'grace\s+period',
            r'grace\s+period\s+of\s+\d+',
            r'\d+\s+days?\s+grace',
            r'grace.*?\d+\s+days?',
            r'renewal.*?grace',
            r'premium.*?grace.*?period'
        ],
        'waiting': [
            r'waiting\s+period',
            r'\d+\s+(months?|years?)\s+waiting',
            r'waiting.*?\d+\s+(months?|years?)',
            r'exclusion.*?waiting',
            r'covered\s+after.*?\d+\s+(months?|years?)'
        ],
        'maternity': [
            r'maternity\s+expenses?',
            r'maternity\s+benefits?',
            r'delivery.*?expenses?',
            r'pregnancy.*?coverage',
            r'childbirth.*?expenses?'
        ],
        'cataract': [
            r'cataract\s+surgery',
            r'cataract.*?waiting',
            r'eye\s+surgery',
            r'cataract.*?\d+\s+(months?|years?)'
        ],
        'discount': [
            r'no\s+claim\s+discount',
            r'ncd',
            r'discount.*?\d+%',
            r'claim\s+free.*?discount'
        ],
        'ayush': [
            r'ayush\s+treatment',
            r'ayurveda',
            r'homeopathy',
            r'unani',
            r'siddha',
            r'alternative\s+medicine'
        ],
        'hospital': [
            r'hospital\s+means',
            r'hospital.*?definition',
            r'hospital.*?established',
            r'institution.*?hospital',
            r'hospital.*?registered'
        ],
        'room': [
            r'room\s+rent',
            r'room\s+charges',
            r'icu\s+charges',
            r'intensive\s+care',
            r'\d+%.*?room',
            r'room.*?\d+%'
        ]
    }

    boost = 0
    for key, patterns in insurance_patterns.items():
        if key in q_lower:
            for pattern in patterns:
                if re.search(pattern, c_lower):
                    boost += 0.8  # Higher boost for pattern matches
                    break
            else:
                # Fallback to simple keyword match
                if key in c_lower:
                    boost += 0.3

    # Enhanced numerical and percentage matching
    if re.search(r'\d+\s*(days?|months?|years?|%)', c_lower):
        boost += 0.3

    # Boost for definition patterns
    if re.search(r'(means?|definition|refers?\s+to|defined\s+as)', c_lower):
        boost += 0.2

    # Boost for section headers and important markers
    if re.search(r'(section|clause|article|\d+\.\d+)', c_lower):
        boost += 0.1

    return base_score + boost

def find_top_chunks_enhanced(question: str, chunks: List[str], top_k: int = 5) -> List[str]:
    """Enhanced chunk retrieval with better scoring and more chunks"""
    if not chunks:
        return []

    # Parallel scoring for speed
    def score_chunk(chunk):
        return enhanced_chunk_scoring(question, chunk)

    # Score chunks in parallel
    with ThreadPoolExecutor(max_workers=4) as executor:
        scores = list(executor.map(score_chunk, chunks))

    # Get top chunks with minimum score threshold
    scored_chunks = [(score, chunk) for score, chunk in zip(scores, chunks) if score > 0.1]
    scored_chunks.sort(reverse=True, key=lambda x: x[0])

    # Return more chunks for better context
    top_chunks = [chunk for _, chunk in scored_chunks[:top_k]]

    # If we don't have enough high-scoring chunks, add some medium-scoring ones
    if len(top_chunks) < 3:
        medium_chunks = [(score, chunk) for score, chunk in zip(scores, chunks) if 0.05 <= score <= 0.1]
        medium_chunks.sort(reverse=True, key=lambda x: x[0])
        additional_needed = min(3 - len(top_chunks), len(medium_chunks))
        top_chunks.extend([chunk for _, chunk in medium_chunks[:additional_needed]])

    return top_chunks

def create_enhanced_prompt(questions: List[str], context_map: Dict[str, List[str]]) -> List[Dict]:
    """Create enhanced prompt with better context assembly"""
    question_contexts = []

    for i, question in enumerate(questions, 1):
        # Use more chunks for better context
        contexts = context_map.get(question, [])[:4]  # Increased from 2 to 4

        if contexts:
            # Number contexts for clarity
            numbered_contexts = []
            for j, context in enumerate(contexts, 1):
                numbered_contexts.append(f"Context {j}: {context}")

            context_text = "\n\n".join(numbered_contexts)
        else:
            context_text = "No relevant context found."

        question_contexts.append(f"Question {i}: {question}\n{context_text}\n")

    # Enhanced prompt with better instructions
    batch_prompt = f"""You are an insurance policy expert. Answer each question using ONLY the provided context. 

IMPORTANT INSTRUCTIONS:
- If the context contains the answer, provide it with specific details
- If the context lacks sufficient information, say "Information not available in the provided context"
- Quote relevant parts from the context when possible
- Be precise with numbers, percentages, and time periods
- For definitions, provide the complete definition if available

Format your answers as: A1: [answer] A2: [answer] A3: [answer]...

{chr(10).join(question_contexts)}"""

    return [
        {"role": "system", "content": "You are an expert insurance policy analyst. Provide accurate, detailed answers based strictly on the provided context."},
        {"role": "user", "content": batch_prompt}
    ]

async def call_api_enhanced(messages: List[Dict], max_tokens: int = 1200) -> str:
    """Enhanced API call with better settings"""
    if not LLM_KEY:
        raise HTTPException(status_code=500, detail="HF_TOKEN not set")

    rate_limiter.acquire()

    API_URL = "https://router.huggingface.co/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {LLM_KEY}",
        "Content-Type": "application/json"
    }

    # Enhanced settings for better accuracy
    payload = {
        "messages": messages,
        "model": "mistralai/Mistral-7B-Instruct-v0.2:featherless-ai",
        "max_tokens": max_tokens,  # Increased for detailed answers
        "temperature": 0.1,  # Slightly higher for better reasoning
        "top_p": 0.2,
        "stream": False
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)

        if response.status_code == 429:
            await asyncio.sleep(3)
            response = requests.post(API_URL, headers=headers, json=payload, timeout=30)

        response.raise_for_status()
        result = response.json()

        if 'choices' in result and len(result['choices']) > 0:
            return result['choices'][0]['message']['content']
        else:
            raise HTTPException(status_code=500, detail="Unexpected API response")

    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"API failed: {str(e)}")

def parse_answers_enhanced(response: str, expected_count: int) -> List[str]:
    """Enhanced answer parsing with better pattern matching"""
    # Try multiple patterns for answer extraction
    patterns = [
        r'A(\d+):\s*(.+?)(?=A\d+:|$)',  # A1: answer
        r'Answer\s+(\d+):\s*(.+?)(?=Answer\s+\d+:|$)',  # Answer 1: answer
        r'(\d+)\)\s*(.+?)(?=\d+\)|$)',  # 1) answer
        r'Question\s+\d+.*?:\s*(.+?)(?=Question\s+\d+|$)'  # Question 1: answer
    ]

    answers = ["Information not available in the provided context."] * expected_count

    for pattern in patterns:
        matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)

        if matches:
            for match in matches:
                try:
                    if len(match) == 2:  # Pattern with number and answer
                        answer_num = int(match[0]) - 1
                        answer_text = match[1].strip()
                    else:  # Pattern with just answer
                        answer_num = len([a for a in answers if a != "Information not available in the provided context."])
                        answer_text = match[0].strip()

                    if 0 <= answer_num < expected_count and answer_text:
                        answers[answer_num] = answer_text
                except (ValueError, IndexError):
                    continue
            break  # Use first successful pattern

    return answers

@app.get("/")
async def root():
    return {
        "message": "Insurance Claims Processing API - Enhanced",
        "version": "4.0.0",
        "improvements": [
            "Complete document processing (all pages)",
            "Enhanced chunking with semantic boundaries", 
            "Better keyword matching for insurance terms",
            "Multi-level retrieval strategy",
            "Improved context assembly",
            "Enhanced handling of specific terms"
        ]
    }

@app.post("/api/v1/hackrx/run")
async def document_qa_enhanced(
    req: QARequest,
    token: str = Depends(verify_bearer_token)
):
    """Enhanced Document Q&A with improved PDF parsing and retrieval"""
    start_time = time.time()

    try:
        print(f"ðŸš€ Enhanced processing of {len(req.questions)} questions")

        # Step 1: Enhanced PDF extraction (all pages)
        extraction_tasks = [extract_pdf_enhanced(doc) for doc in req.documents]
        pdf_texts = await asyncio.gather(*extraction_tasks)

        # Step 2: Enhanced chunking
        all_chunks = []
        for text in pdf_texts:
            chunks = enhanced_chunking(text, chunk_size=1500, overlap=300)
            all_chunks.extend(chunks)

        print(f"ðŸ“š Created {len(all_chunks)} enhanced chunks")

        # Memory cleanup
        del pdf_texts
        gc.collect()

        # Step 3: Enhanced chunk finding
        async def find_chunks_for_question(question):
            return find_top_chunks_enhanced(question, all_chunks, top_k=5)

        chunk_tasks = [find_chunks_for_question(q) for q in req.questions]
        chunk_results = await asyncio.gather(*chunk_tasks)

        context_map = dict(zip(req.questions, chunk_results))

        # Debug: Print context for grace period question
        for question in req.questions:
            if 'grace' in question.lower():
                print(f"ðŸ” Grace period question: {question}")
                print(f"ðŸ“„ Found {len(context_map[question])} contexts")
                for i, context in enumerate(context_map[question][:2]):
                    print(f"Context {i+1}: {context[:200]}...")

        # Step 4: Enhanced API call
        messages = create_enhanced_prompt(req.questions, context_map)
        batch_response = await call_api_enhanced(messages)

        # Step 5: Enhanced answer parsing
        answers = parse_answers_enhanced(batch_response, len(req.questions))

        elapsed_time = time.time() - start_time
        print(f"âœ… Completed in {elapsed_time:.2f} seconds")

        return {"answers": answers}

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    global http_client
    if http_client:
        await http_client.aclose()

if __name__ == "__main__":
    print("ðŸš€ Starting Enhanced Insurance API...")
    print("ðŸ“ˆ Improved PDF parsing and retrieval")
    print("ðŸŽ¯ Target: 90%+ accuracy")

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
